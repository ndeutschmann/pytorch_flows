{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import pi,sqrt,log,e\n",
    "from time import time\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing jacobians for simple transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.flows.sampling import FactorizedFlowPrior\n",
    "from src.models.flows.backprop_jacobian_flows.simple_backprop_flows import LinearFlow,SigmoidFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated Jacobian:  -5.124557971954346\n",
      "analytic Jacobian  :  -5.124558448791504\n"
     ]
    }
   ],
   "source": [
    "prior_mu =  torch.tensor(0.).to(device)\n",
    "prior_sig =  torch.tensor(100.).to(device)\n",
    "prior = torch.distributions.normal.Normal(prior_mu,prior_sig)\n",
    "\n",
    "sampler = FactorizedFlowPrior(d=2,prior_1d=prior)\n",
    "nnflow = LinearFlow(d=2)\n",
    "\n",
    "nnflow.cuda(device=device)\n",
    "\n",
    "nbatch=100000\n",
    "zj = sampler(nbatch)\n",
    "xlj = nnflow(zj)\n",
    "j = torch.exp(xlj[:,-1])\n",
    "v,r=torch.var_mean(j)\n",
    "v=v.detach().cpu().item()\n",
    "r=r.detach().cpu().item()\n",
    "print(\"calculated Jacobian: \",torch.mean(xlj[:,-1]-(zj[:,-1])).detach().cpu().item())\n",
    "print(\"analytic Jacobian  : \",torch.log(torch.abs(torch.det(nnflow.flow.weight.data))).cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral of 1 sampled non-uniformly: 1.0034523010253906+/-0.009146783172594812\n",
      "computed Jacobian:  tensor([2.2371e-02, 7.7865e-03, 6.9855e-05,  ..., 2.6944e-02, 9.9202e-03,\n",
      "        8.7188e-03], device='cuda:1', grad_fn=<ExpBackward>)\n",
      "analytic Jacobian:  tensor([2.2371e-02, 7.7865e-03, 6.9855e-05,  ..., 2.6944e-02, 9.9202e-03,\n",
      "        8.7188e-03], device='cuda:1', grad_fn=<MulBackward0>)\n",
      "All relative differences < .1%:  True\n"
     ]
    }
   ],
   "source": [
    "prior_mu =  torch.tensor(0.).to(device)\n",
    "prior_sig =  torch.tensor(3.).to(device)\n",
    "prior = torch.distributions.normal.Normal(prior_mu,prior_sig)\n",
    "\n",
    "sampler = FactorizedFlowPrior(d=2,prior_1d=prior)\n",
    "nnflow = SigmoidFlow(d=2)\n",
    "\n",
    "nnflow.cuda(device=device)\n",
    "\n",
    "nbatch=10000\n",
    "zj = sampler(nbatch)\n",
    "xlj = nnflow(zj)\n",
    "j = torch.exp(xlj[:,-1])\n",
    "v,r=torch.var_mean(j)\n",
    "v=v.detach().cpu().item()\n",
    "r=r.detach().cpu().item()\n",
    "print(\"Integral of 1 sampled non-uniformly: {}+/-{}\".format(r,sqrt(v/nbatch)))\n",
    "sj1=torch.exp(xlj[:,-1]-zj[:,-1])\n",
    "sj2=xlj[:,0]*(1-xlj[:,0])*xlj[:,1]*(1-xlj[:,1])\n",
    "print(\"computed Jacobian: \",sj1)\n",
    "print(\"analytic Jacobian: \",sj2)\n",
    "print(\"All relative differences < .1%: \",torch.all(torch.abs(sj1-sj2)/sj1 < 1.e-3).cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining two transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral of 1 sampled non-uniformly: 0.9927786588668823+/-0.007982595585108306\n"
     ]
    }
   ],
   "source": [
    "sig=1.\n",
    "prior_mu =  torch.tensor(0.).to(device)\n",
    "prior_sig =  torch.tensor(sig).to(device)\n",
    "prior = torch.distributions.normal.Normal(prior_mu,prior_sig)\n",
    "\n",
    "sampler = FactorizedFlowPrior(d=2,prior_1d=prior)\n",
    "lflow = LinearFlow(d=2)\n",
    "lflow.weight_init_identity_(0.001)\n",
    "sflow = SigmoidFlow(d=2)\n",
    "\n",
    "sampler.cuda(device=device)\n",
    "lflow.cuda(device=device)\n",
    "sflow.cuda(device=device)\n",
    "\n",
    "nbatch=10000000\n",
    "\n",
    "zj = sampler(nbatch)\n",
    "yj = lflow(zj)\n",
    "xj = sflow(yj)\n",
    "j = torch.exp(xj[:,-1])\n",
    "v,r=torch.var_mean(j)\n",
    "v=v.detach().cpu().item()\n",
    "r=r.detach().cpu().item()\n",
    "print(\"Integral of 1 sampled non-uniformly: {}+/-{}\".format(r,sqrt(v/nbatch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9972, 0.9972, 0.9972,  ..., 0.9972, 0.9972, 0.9972], device='cuda:1',\n",
      "       grad_fn=<ExpBackward>)\n",
      "tensor(0.9972, device='cuda:1')\n",
      "tensor([0.0189, 0.0508, 0.0396,  ..., 0.0271, 0.0206, 0.0616], device='cuda:1',\n",
      "       grad_fn=<ExpBackward>)\n",
      "tensor([0.0189, 0.0508, 0.0396,  ..., 0.0271, 0.0206, 0.0616], device='cuda:1',\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([94.9245,  9.5958, 16.4748,  ..., 37.9965, 74.4941,  6.4728],\n",
      "       device='cuda:1')\n",
      "tensor([94.9245,  9.5958, 16.4748,  ..., 37.9965, 74.4941,  6.4728],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# Checking gradients\n",
    "def normal(x,s):\n",
    "    return(torch.exp(-(x/s)**2/2.)/sqrt(2.*pi*s**2))\n",
    "\n",
    "print(torch.exp(yj[:,-1] - zj[:,-1]))\n",
    "print(torch.abs(torch.det(lflow.flow.weight.data)))\n",
    "\n",
    "print(torch.exp(xj[:,-1] - yj[:,-1]))\n",
    "print(xj[:,0]*(1-xj[:,0])*xj[:,1]*(1-xj[:,1]))\n",
    "\n",
    "print(torch.exp(zj[:,-1]))\n",
    "print(torch.abs(1/(normal(zj[:,0],sig)*normal(zj[:,1],sig))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
